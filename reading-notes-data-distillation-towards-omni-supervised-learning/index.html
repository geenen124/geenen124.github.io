<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" class="h-full" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D,%22class%22:%7B%22ssr%22:%22h-full%22%7D%7D">
  <head>
    <title>Shift Reducer - Reading Notes - Data Distillation: Towards Omni-Supervised Learning </title><meta name="gridsome:hash" content="a7a5687e4a99b1b283ea7ac780b7b3d6e8296ff0"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.15"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" data-key="description" name="description" content="Radosavovic, I et. al. Data Distillation: Towards Omni-Supervised Learning (2017)"><meta data-vue-tag="ssr" property="og:type" content="article"><meta data-vue-tag="ssr" property="og:title" content="Reading Notes - Data Distillation: Towards Omni-Supervised Learning"><meta data-vue-tag="ssr" property="og:description" content="Radosavovic, I et. al. Data Distillation: Towards Omni-Supervised Learning (2017)"><meta data-vue-tag="ssr" property="og:url" content="https://shiftreducer.com/reading-notes-data-distillation-towards-omni-supervised-learning/"><meta data-vue-tag="ssr" property="article:published_time" content="2020-07-31"><meta data-vue-tag="ssr" property="og:image" content="https://shiftreducer.com/images/bleda-card.png"><meta data-vue-tag="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-tag="ssr" name="twitter:title" content="Reading Notes - Data Distillation: Towards Omni-Supervised Learning"><meta data-vue-tag="ssr" name="twitter:description" content="Radosavovic, I et. al. Data Distillation: Towards Omni-Supervised Learning (2017)"><meta data-vue-tag="ssr" name="twitter:site" content="@cossssmin"><meta data-vue-tag="ssr" name="twitter:creator" content="@cossssmin"><meta data-vue-tag="ssr" name="twitter:image" content="https://shiftreducer.com/images/bleda-card.png"><link data-vue-tag="ssr" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:400,700%7CCardo"><link rel="preload" href="/assets/css/1.styles.a21f5285.css" as="style"><link rel="preload" href="/assets/js/app.79ef14c1.js" as="script"><link rel="preload" href="/assets/js/page--src--templates--post-vue.eda71775.js" as="script"><link rel="prefetch" href="/assets/js/10.e56d6372.js"><link rel="prefetch" href="/assets/js/page--src--pages--404-vue.eba350d6.js"><link rel="prefetch" href="/assets/js/page--src--pages--about-vue.480d3560.js"><link rel="prefetch" href="/assets/js/page--src--pages--index-vue.eb1acfef.js"><link rel="prefetch" href="/assets/js/page--src--templates--author-vue.88441fdd.js"><link rel="prefetch" href="/assets/js/page--src--templates--tag-vue.2d3c4414.js"><link rel="prefetch" href="/assets/js/vendors~page--src--pages--index-vue~page--src--templates--author-vue~page--src--templates--post-vue~~a0fae544.8dd3f6de.js"><link rel="prefetch" href="/assets/js/vendors~page--src--templates--post-vue.002e04a6.js"><link rel="stylesheet" href="/assets/css/1.styles.a21f5285.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body class="antialiased font-serif" data-vue-tag="%7B%22class%22:%7B%22ssr%22:%22antialiased%20font-serif%22%7D%7D">
    <div data-server-rendered="true" id="app"><main><header><div class="pt-24"><div class="max-w-xl md:max-w-3xl xl:max-w-4xl mx-auto text-center px-6"><p class="text-gray-700 text-xs mb-2 uppercase">3 min read</p><h1 class="text-3xl sm:text-5xl leading-tight font-sans font-bold mb-2 text-black">Reading Notes - Data Distillation: Towards Omni-Supervised Learning</h1><p class="text-gray-700"><span><a href="/author/Alex%20Geenen/" class="text-gray-700 capitalize border-b border-transparent hover:border-gray-400 transition-colors duration-300">Alex Geenen</a> •
        </span><time datetime="2020-07-31 00:00:00" class="capitalize">31 July, 2020</time></p></div></div><nav class="absolute top-0 left-0 z-20 mt-6 ml-6"><a href="/" class="text-sm border opacity-75 hover:opacity-100 rounded-full px-4 py-2 transition-opacity duration-300 active text-gray-900 border-gray-400">← Home</a></nav></header><article class="max-w-xl md:max-w-2xl xl:max-w-3xl mx-auto px-6 sm:px-12 pt-16"><!----><div class="markdown text-lg leading-normal text-gray-700 pb-10"><h4 id="omni-supervised-learning"><a href="#omni-supervised-learning" aria-hidden="true"><span class="icon icon-link"></span></a><a href="https://arxiv.org/abs/1712.04440" target="_blank" rel="nofollow noopener noreferrer">Omni-Supervised Learning</a></h4>
<p>This paper describes a novel method for omni-supervised learning (a variant of semi-supervised learning), which the authors call Data Distillation. </p>
<ul>
<li>
<p>Data Distillation borrows from the student-teacher dynamics of Model Distillation. </p>
<ul>
<li>An aside: Model Distillation is a technique where we aim to teach a less computationally expensive ''student'' model to mimic the behavior of a complex (and usually pretty computationally expensive) ''teacher'' model. In many multi-target tasks, this consists of training the output distribution of the student to match the teacher's output distribution. This way, we aren't just teaching it about the relevant target (in the case of classification) - but also about the long-tail, which is where a lot of the value of the ''teacher'' can lie. This long tail can be seen as the most difficult-to-learn information contained within these complex models.</li>
</ul>
</li>
<li>Instead of using a pre-trained expert model to teach the student, they pick a more indirect path - in which the student has the exact same architecture as the teacher.</li>
<li>
<p>Data Distillation works as follows: </p>
<ul>
<li>The teacher network is trained using the normal supervised dataset for the desired task.</li>
<li>New unlabeled data is presented to the now-trained teacher network, which predicts a label for each new sample. The unlabeled data is evaluated multiple times by the teacher - each time with a different transform applied (e.g. in the case of computer vision this could consist of rotating, scaling, and/or cropping an image). </li>
<li>The set of labels per unlabeled data point are seen as outputs from models in an ensemble network, and they are then aggregated to produce a final label.</li>
<li>The student network - architecturally identical to the teacher network - is trained from scratch on the union of the supervised dataset and this newly ''self''-labeled dataset.</li>
</ul>
</li>
<li><em>Why does this work?</em> Since many neural models have achieved very good results on computer-vision tasks, the authors argue that using the same network to augment the data used for training no longer just introduces noise into the dataset, but actually introduces new knowledge.</li>
</ul>
<h4 id="things-to-note"><a href="#things-to-note" aria-hidden="true"><span class="icon icon-link"></span></a>Things To Note:</h4>
<ul>
<li>
<p>The output of the ensemble needs to be a hard label - averaging won't be enough, since it produces a ''soft'' label i.e. a probability vector.</p>
<ul>
<li>The generation of hard labels requires task and dataset-specific logic. In the case of the COCO tasks used by the authors, they apply <a href="https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/" target="_blank" rel="nofollow noopener noreferrer">Non-maximum Suppression</a> in order to merge multiple sets of boxes. They also threshold the boxes - boxes with a probability below this threshold are not considered.</li>
</ul>
</li>
<li>The data needs to be managed when retraining. The student model is trained on a union of labeled and unlabeled data, and the authors apply a fixed sampling ratio between supervised and generated examples at a minibatch level. This is done in order to ensure that the quality supervised data is properly represented in every minibatch. The reported ratio is 6:4 original to teacher-labeled images.</li>
<li>When the system was tested on a small-scale dataset (Table 1a), the data-distilled student outperformed the smaller supervised dataset. However, when compared to a ''vanilla'' model trained on a supervised dataset that was the same size as union of the small dataset and the augmented data, it was outperformed by the vanilla model. The authors argue that the performance of a fully-supervised model on the small-scale dataset can be seen as the lower bound on the performance of omni-supervised models, since there is much more unlabeled data available.</li>
</ul>
</div><footer class="flex flex-wrap pb-10 sm:pb-16"><div><a href="/tag/semi-supervised/" class="inline-block text-teal-400 hover:text-white hover:bg-teal-400 border border-teal-400 font-sans font-bold text-xs sm:text-sm px-4 py-2 mr-4 mb-2 rounded-full transition-colors duration-300"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" role="img" class="inline w-3 fill-current align-middle mr-1"><path d="M0 10V2l2-2h8l10 10-10 10L0 10zm4.5-4a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3z"></path></svg>
          semi-supervised
          </a><a href="/tag/object-detection/" class="inline-block text-teal-400 hover:text-white hover:bg-teal-400 border border-teal-400 font-sans font-bold text-xs sm:text-sm px-4 py-2 mr-4 mb-2 rounded-full transition-colors duration-300"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" role="img" class="inline w-3 fill-current align-middle mr-1"><path d="M0 10V2l2-2h8l10 10-10 10L0 10zm4.5-4a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3z"></path></svg>
          object-detection
          </a><a href="/tag/computer-vision/" class="inline-block text-teal-400 hover:text-white hover:bg-teal-400 border border-teal-400 font-sans font-bold text-xs sm:text-sm px-4 py-2 mr-4 mb-2 rounded-full transition-colors duration-300"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" role="img" class="inline w-3 fill-current align-middle mr-1"><path d="M0 10V2l2-2h8l10 10-10 10L0 10zm4.5-4a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3z"></path></svg>
          computer-vision
          </a><a href="/tag/reading%20notes/" class="inline-block text-teal-400 hover:text-white hover:bg-teal-400 border border-teal-400 font-sans font-bold text-xs sm:text-sm px-4 py-2 mr-4 mb-2 rounded-full transition-colors duration-300"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" role="img" class="inline w-3 fill-current align-middle mr-1"><path d="M0 10V2l2-2h8l10 10-10 10L0 10zm4.5-4a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3z"></path></svg>
          reading notes
          </a></div></footer></article><footer class="text-gray-700 text-sm leading-normal flex flex-wrap justify-between mx-auto max-w-3xl px-6 sm:px-12 pb-8 sm:pb-10"><div class="w-full sm:w-1/2 mb-4 sm:mb-0"></div><div class="w-full sm:w-1/2"><nav><ul class="flex sm:justify-end -mx-2"><li class="px-2"><a href="/" class="border-b border-transparent hover:border-gray-400 transition-colors duration-300 active">Home</a></li><li class="px-2"><a href="/about/" class="border-b border-transparent hover:border-gray-400 transition-colors duration-300">About</a></li></ul></nav></div></footer></main></div>
    <script>window.__INITIAL_STATE__={"data":{"post":{"title":"Reading Notes - Data Distillation: Towards Omni-Supervised Learning","path":"\u002Freading-notes-data-distillation-towards-omni-supervised-learning\u002F","slug":"reading-notes-omni-supervised-learning","datetime":"2020-07-31 00:00:00","content":"\u003Ch4 id=\"omni-supervised-learning\"\u003E\u003Ca href=\"#omni-supervised-learning\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1712.04440\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EOmni-Supervised Learning\u003C\u002Fa\u003E\u003C\u002Fh4\u003E\n\u003Cp\u003EThis paper describes a novel method for omni-supervised learning (a variant of semi-supervised learning), which the authors call Data Distillation. \u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EData Distillation borrows from the student-teacher dynamics of Model Distillation. \u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EAn aside: Model Distillation is a technique where we aim to teach a less computationally expensive ''student'' model to mimic the behavior of a complex (and usually pretty computationally expensive) ''teacher'' model. In many multi-target tasks, this consists of training the output distribution of the student to match the teacher's output distribution. This way, we aren't just teaching it about the relevant target (in the case of classification) - but also about the long-tail, which is where a lot of the value of the ''teacher'' can lie. This long tail can be seen as the most difficult-to-learn information contained within these complex models.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EInstead of using a pre-trained expert model to teach the student, they pick a more indirect path - in which the student has the exact same architecture as the teacher.\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EData Distillation works as follows: \u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EThe teacher network is trained using the normal supervised dataset for the desired task.\u003C\u002Fli\u003E\n\u003Cli\u003ENew unlabeled data is presented to the now-trained teacher network, which predicts a label for each new sample. The unlabeled data is evaluated multiple times by the teacher - each time with a different transform applied (e.g. in the case of computer vision this could consist of rotating, scaling, and\u002For cropping an image). \u003C\u002Fli\u003E\n\u003Cli\u003EThe set of labels per unlabeled data point are seen as outputs from models in an ensemble network, and they are then aggregated to produce a final label.\u003C\u002Fli\u003E\n\u003Cli\u003EThe student network - architecturally identical to the teacher network - is trained from scratch on the union of the supervised dataset and this newly ''self''-labeled dataset.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EWhy does this work?\u003C\u002Fem\u003E Since many neural models have achieved very good results on computer-vision tasks, the authors argue that using the same network to augment the data used for training no longer just introduces noise into the dataset, but actually introduces new knowledge.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch4 id=\"things-to-note\"\u003E\u003Ca href=\"#things-to-note\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EThings To Note:\u003C\u002Fh4\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EThe output of the ensemble needs to be a hard label - averaging won't be enough, since it produces a ''soft'' label i.e. a probability vector.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EThe generation of hard labels requires task and dataset-specific logic. In the case of the COCO tasks used by the authors, they apply \u003Ca href=\"https:\u002F\u002Fwww.pyimagesearch.com\u002F2014\u002F11\u002F17\u002Fnon-maximum-suppression-object-detection-python\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003ENon-maximum Suppression\u003C\u002Fa\u003E in order to merge multiple sets of boxes. They also threshold the boxes - boxes with a probability below this threshold are not considered.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EThe data needs to be managed when retraining. The student model is trained on a union of labeled and unlabeled data, and the authors apply a fixed sampling ratio between supervised and generated examples at a minibatch level. This is done in order to ensure that the quality supervised data is properly represented in every minibatch. The reported ratio is 6:4 original to teacher-labeled images.\u003C\u002Fli\u003E\n\u003Cli\u003EWhen the system was tested on a small-scale dataset (Table 1a), the data-distilled student outperformed the smaller supervised dataset. However, when compared to a ''vanilla'' model trained on a supervised dataset that was the same size as union of the small dataset and the augmented data, it was outperformed by the vanilla model. The authors argue that the performance of a fully-supervised model on the small-scale dataset can be seen as the lower bound on the performance of omni-supervised models, since there is much more unlabeled data available.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n","description":"Radosavovic, I et. al. Data Distillation: Towards Omni-Supervised Learning (2017)","timeToRead":3,"author":{"id":"Alex Geenen","title":"Alex Geenen","path":"\u002Fauthor\u002FAlex%20Geenen\u002F"},"tags":[{"id":"semi-supervised","title":"semi-supervised","path":"\u002Ftag\u002Fsemi-supervised\u002F"},{"id":"object-detection","title":"object-detection","path":"\u002Ftag\u002Fobject-detection\u002F"},{"id":"computer-vision","title":"computer-vision","path":"\u002Ftag\u002Fcomputer-vision\u002F"},{"id":"reading notes","title":"reading notes","path":"\u002Ftag\u002Freading%20notes\u002F"}]}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/assets/js/app.79ef14c1.js" defer></script><script src="/assets/js/page--src--templates--post-vue.eda71775.js" defer></script>
  </body>
</html>

<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" class="h-full" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D,%22class%22:%7B%22ssr%22:%22h-full%22%7D%7D">
  <head>
    <title>Shift Reducer - Posts tagged &quot;Reading Notes&quot;</title><meta name="gridsome:hash" content="5524ebf7238f83aefa98419400c4213244caddbd"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.15"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" data-key="description" name="description" content="Browse posts tagged &quot;Reading Notes&quot;"><meta data-vue-tag="ssr" property="og:type" content="website"><meta data-vue-tag="ssr" property="og:title" content="Posts tagged &quot;Reading Notes&quot;"><meta data-vue-tag="ssr" property="og:description" content="Browse posts tagged &quot;Reading Notes&quot;"><meta data-vue-tag="ssr" property="og:url" content="https://shiftreducer.com/tag/reading%20notes/"><meta data-vue-tag="ssr" property="og:image" content="https://shiftreducer.com/images/bleda-card.png"><meta data-vue-tag="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-tag="ssr" name="twitter:title" content="Posts tagged &quot;Reading Notes&quot;"><meta data-vue-tag="ssr" name="twitter:description" content="Browse posts tagged &quot;Reading Notes&quot;"><meta data-vue-tag="ssr" name="twitter:site" content="@cossssmin"><meta data-vue-tag="ssr" name="twitter:creator" content="@cossssmin"><meta data-vue-tag="ssr" name="twitter:image" content="https://shiftreducer.com/images/bleda-card.png"><link data-vue-tag="ssr" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:400,700%7CCardo"><link rel="preload" href="/assets/css/1.styles.a21f5285.css" as="style"><link rel="preload" href="/assets/js/app.04ee8f12.js" as="script"><link rel="preload" href="/assets/js/page--src--templates--tag-vue.269efff5.js" as="script"><link rel="prefetch" href="/assets/js/10.e56d6372.js"><link rel="prefetch" href="/assets/js/page--src--pages--404-vue.eba350d6.js"><link rel="prefetch" href="/assets/js/page--src--pages--about-vue.480d3560.js"><link rel="prefetch" href="/assets/js/page--src--pages--index-vue.0d7ad97a.js"><link rel="prefetch" href="/assets/js/page--src--templates--author-vue.88420af0.js"><link rel="prefetch" href="/assets/js/page--src--templates--post-vue.92e4651d.js"><link rel="prefetch" href="/assets/js/vendors~page--src--pages--index-vue~page--src--templates--author-vue~page--src--templates--post-vue~~a0fae544.8dd3f6de.js"><link rel="prefetch" href="/assets/js/vendors~page--src--templates--post-vue.002e04a6.js"><link rel="stylesheet" href="/assets/css/1.styles.a21f5285.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body class="antialiased font-serif" data-vue-tag="%7B%22class%22:%7B%22ssr%22:%22antialiased%20font-serif%22%7D%7D">
    <div data-server-rendered="true" id="app"><main><header><div class="max-w-xl md:max-w-3xl xl:max-w-4xl flex flex-col-reverse mx-auto text-center px-6 pt-24 pb-10 md:py-32 border-b border-gray-300"><p class="text-gray-700 leading-normal">1 posts in total</p><h1 class="text-4xl sm:text-5xl md:text-6xl font-sans font-bold mb-2 capitalize">Reading Notes</h1><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" role="img" aria-labelledby="tagIcon" class="w-5 sm:w-6 fill-current text-gray-500 mx-auto mb-1"><title id="tagIcon">Posts tagged</title><path d="M0 10V2l2-2h8l10 10-10 10L0 10zm4.5-4a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3z"></path></svg></div><nav class="absolute top-0 left-0 z-20 mt-6 ml-6"><a href="/" class="text-sm border text-gray-900 border-gray-400 opacity-75 hover:opacity-100 rounded-full px-4 py-2 transition-opacity duration-300 active">← Home</a></nav></header><section><article><div class="mx-auto max-w-3xl px-6"><div class="py-8 sm:py-20 border-b border-gray-300"><header class="text-center mb-8"><time datetime="2020-07-31 00:00:00" class="text-gray-700 text-xs mb-2 uppercase">31 July, 2020</time><h2 class="text-3xl sm:text-4xl leading-tight font-sans mb-1 sm:mb-2"><a href="/omni-supervised-learning-improving-supervised-models-with-unlabeled-data/" class="text-black font-bold">Omni-Supervised Learning - Improving Supervised Models With Unlabeled Data</a></h2><p class="text-gray-700 leading-normal text-sm sm:text-base"><span>by <a href="/author/Alex%20Geenen/" class="text-gray-700 capitalize border-b border-transparent hover:border-gray-400 transition-colors duration-300">Alex Geenen</a></span><!----><span> · </span><span>3 min read</span></p></header><p class="leading-normal text-gray-700 text-lg px-2 sm:px-4 md:px-10">Radosavovic, I et. al. Data Distillation: Towards Omni-Supervised Learning (2017)</p></div></div></article></section><!----><footer class="text-gray-700 text-sm leading-normal flex flex-wrap justify-between mx-auto max-w-3xl px-6 sm:px-12 py-8 sm:py-16"><div class="w-full sm:w-1/2 mb-4 sm:mb-0"></div><div class="w-full sm:w-1/2"><nav><ul class="flex sm:justify-end -mx-2"><li class="px-2"><a href="/" class="border-b border-transparent hover:border-gray-400 transition-colors duration-300 active">Home</a></li><li class="px-2"><a href="/about/" class="border-b border-transparent hover:border-gray-400 transition-colors duration-300">About</a></li></ul></nav></div></footer></main></div>
    <script>window.__INITIAL_STATE__={"data":{"tag":{"id":"reading notes","title":"reading notes","path":"\u002Ftag\u002Freading%20notes\u002F","belongsTo":{"totalCount":1,"pageInfo":{"totalPages":1,"currentPage":1},"edges":[{"node":{"id":"7f521547b5f31c89e401a27f9c1fcde4","title":"Omni-Supervised Learning - Improving Supervised Models With Unlabeled Data","datetime":"2020-07-31 00:00:00","path":"\u002Fomni-supervised-learning-improving-supervised-models-with-unlabeled-data\u002F","content":"\u003C!--- #### [Omni-Supervised Learning](https:\u002F\u002Farxiv.org\u002Fabs\u002F1712.04440) ---\u003E\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1712.04440\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EThis paper by Radosavovic et. al.\u003C\u002Fa\u003E describes a novel method for omni-supervised learning (a variant of semi-supervised learning), which the authors call Data Distillation. This new approach can be used when you have a supervised neural model that is already reliably performant. It allows you to leverage unlabeled data to further improve the performance of your supervised model - by letting the model label new examples, which it can then learn from.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"display:block;text-align:center\"\u003E\u003Cimg title=\"Model Distillation - from Radosavovic et. al. 2017\" style=\"margin-left:auto;margin-right:auto\" src=\"\u002Fimages\u002Fposts\u002Fmodel_distillation.png\" \u002F\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EData Distillation borrows from the student-teacher dynamics of \u003Ca href=\"https:\u002F\u002Fheartbeat.fritz.ai\u002Fresearch-guide-model-distillation-techniques-for-deep-learning-4a100801c0eb\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EModel Distillation\u003C\u002Fa\u003E. \u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EAn aside: \u003Ca href=\"https:\u002F\u002Fheartbeat.fritz.ai\u002Fresearch-guide-model-distillation-techniques-for-deep-learning-4a100801c0eb\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EModel Distillation\u003C\u002Fa\u003E is a technique where we aim to teach a less computationally expensive ''student'' model to mimic the behavior of a complex (and usually pretty computationally expensive) ''teacher'' model. In many multi-target tasks, this consists of training the output distribution of the student to match the teacher's output distribution. This way, we aren't just teaching it about the relevant target (in the case of classification) - but also about the long-tail, which is where a lot of the value of the ''teacher'' can lie. This long tail can be seen as the most difficult-to-learn information contained within these complex models.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EInstead of using a pre-trained expert model to teach the student, they pick a more indirect path - in which the student has the exact same architecture as the teacher.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cspan style=\"display:block;text-align:center\"\u003E\u003Cimg title=\"Data Distillation - from Radosavovic et. al. 2017\" style=\"margin-left:auto;margin-right:auto\" src=\"\u002Fimages\u002Fposts\u002Fdata_distillation.png\" \u002F\u003E\u003C\u002Fspan\u003E\nData Distillation works as follows: \u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EThe teacher network is trained using the normal supervised dataset for the desired task.\u003C\u002Fli\u003E\n\u003Cli\u003ENew unlabeled data is presented to the now-trained teacher network, which predicts a label for each new sample. The unlabeled data is evaluated multiple times by the teacher - each time with a different transform applied (e.g. in the case of computer vision this could consist of rotating, scaling, and\u002For cropping an image). \u003C\u002Fli\u003E\n\u003Cli\u003EThe set of labels per unlabeled data point are seen as outputs from models in an ensemble network, and they are then aggregated to produce a final label.\u003C\u002Fli\u003E\n\u003Cli\u003EThe student network - architecturally identical to the teacher network - is trained from scratch on the union of the supervised dataset and this newly ''self''-labeled dataset.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cem\u003EWhy does this work?\u003C\u002Fem\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ESince many neural models have achieved very good results on computer-vision tasks, the authors argue that using the same network to augment the data used for training no longer just introduces noise into the dataset, but actually introduces new knowledge.\u003C\u002Fp\u003E\n\u003Ch4 id=\"things-to-note\"\u003E\u003Ca href=\"#things-to-note\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EThings To Note:\u003C\u002Fh4\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cp\u003EThe output of the ensemble needs to be a hard label - averaging won't be enough, since it produces a ''soft'' label i.e. a probability vector.\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EThe generation of hard labels requires task and dataset-specific logic. In the case of the COCO tasks used by the authors, they apply \u003Ca href=\"https:\u002F\u002Fwww.pyimagesearch.com\u002F2014\u002F11\u002F17\u002Fnon-maximum-suppression-object-detection-python\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003ENon-maximum Suppression\u003C\u002Fa\u003E in order to merge multiple sets of boxes. They also threshold the boxes - boxes with a probability below this threshold are not considered.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EThe data needs to be managed when retraining. The student model is trained on a union of labeled and unlabeled data, and the authors apply a fixed sampling ratio between supervised and generated examples at a minibatch level. This is done in order to ensure that the quality supervised data is properly represented in every minibatch. The reported ratio is 6:4 original to teacher-labeled images.\n\u003Cspan style=\"display:block;text-align:center\"\u003E\u003Cimg title=\"Table 1a results - from Radosavovic et. al. 2017\" style=\"margin-left:auto;margin-right:auto\" src=\"\u002Fimages\u002Fposts\u002Ftable_1a.png\" \u002F\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\n\u003Cli\u003EWhen the system was tested on a small-scale dataset (Table a), the data-distilled student outperformed the smaller supervised dataset. However, when compared to a ''vanilla'' model trained on a supervised dataset that was the same size as union of the small dataset and the augmented data, it was outperformed by the vanilla model. The authors argue that the performance of a fully-supervised model on the small-scale dataset can be seen as the lower bound on the performance of omni-supervised models, since there is much more unlabeled data available.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n","excerpt":"Radosavovic, I et. al. Data Distillation: Towards Omni-Supervised Learning (2017)","description":"Radosavovic, I et. al. Data Distillation: Towards Omni-Supervised Learning (2017)","timeToRead":3,"author":{"id":"Alex Geenen","title":"Alex Geenen","path":"\u002Fauthor\u002FAlex%20Geenen\u002F"}}}]}}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/assets/js/app.04ee8f12.js" defer></script><script src="/assets/js/page--src--templates--tag-vue.269efff5.js" defer></script>
  </body>
</html>
